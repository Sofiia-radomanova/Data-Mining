# –Ü–º–ø–æ—Ä—Ç –±—ñ–±–ª—ñ–æ—Ç–µ–∫
import nltk
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

print("=== –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –†–ï–°–£–†–°–Ü–í NLTK ===")

# –°–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤ NLTK
required_resources = [
    'punkt',
    'stopwords',
    'wordnet',
    'averaged_perceptron_tagger',
    'maxent_ne_chunker',
    'words',
    'punkt_tab',
    'omw-1.4'  # Open Multilingual WordNet
]

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤
for resource in required_resources:
    try:
        nltk.download(resource, quiet=True)
        print(f"‚úì {resource} –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
    except Exception as e:
        print(f"‚úó –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {resource}: {e}")

print("\n–í—Å—ñ —Ä–µ—Å—É—Ä—Å–∏ NLTK –≥–æ—Ç–æ–≤—ñ –¥–æ —Ä–æ–±–æ—Ç–∏!")

# –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
text = """
The largest bear in the world and the Arctic's top predator, polar bears are a powerful symbol of the strength and endurance of the Arctic. The polar bear's Latin name, Ursus maritimus, means "sea bear." It's an apt name for this majestic species, which spends much of its life in, around, or on the ocean‚Äìpredominantly on or near the sea ice. In the United States, Alaska is home to two polar bear subpopulations.
Considered talented swimmers, polar bears can sustain a pace of six miles per hour by paddling with their front paws and holding their hind legs flat like a rudder. They have a thick layer of body fat and a water-repellent coat that insulates them from the cold air and water.
Polar bears' diet mainly consists of ringed and bearded seals because they need large amounts of fat to survive.
Polar bears rely heavily on sea ice for traveling, hunting, resting, mating and, in some areas, maternal dens. But because of ongoing and potential loss of their sea ice habitat resulting from climate change‚Äìthe primary threat to polar bears Arctic-wide‚Äìpolar bears were listed as a threatened species in the US under the Endangered Species Act in May 2008. As their sea ice habitat recedes earlier in the spring and forms later in the fall, polar bears are increasingly spending longer periods on land, where they are often attracted to areas where humans live.
The survival and protection of the polar bear habitat are urgent issues for WWF. The International Union for the Conservation of Nature (IUCN) Polar Bear Specialist Group releases regular polar bear population updates on the 20 polar bear subpopulations.
"""

print("\n=== –ü–û–ß–ê–¢–ö–û–í–ò–ô –¢–ï–ö–°–¢ ===")
print(text[:300] + "...")


def preprocess_text_nltk(text):
    """–ü–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç—É –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º NLTK"""

    print("\n" + "=" * 60)
    print("–ü–û–ü–ï–†–ï–î–ù–Ø –û–ë–†–û–ë–ö–ê –¢–ï–ö–°–¢–£ –ó NLTK")
    print("=" * 60)

    try:
        # 1. –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è
        from nltk.tokenize import word_tokenize
        tokens = word_tokenize(text)
        print(f"1. –¢–û–ö–ï–ù–Ü–ó–ê–¶–Ü–Ø: {len(tokens)} —Ç–æ–∫–µ–Ω—ñ–≤")
        print(f"   –ü—Ä–∏–∫–ª–∞–¥: {tokens[:12]}...")

        # 2. –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–æ–ø-—Å–ª—ñ–≤ —Ç–∞ –Ω–µ-–ª—ñ—Ç–µ—Ä
        from nltk.corpus import stopwords
        stop_words = set(stopwords.words('english'))
        filtered_tokens = [word for word in tokens if word.isalpha() and word.lower() not in stop_words]
        print(f"2. –ë–ï–ó –°–¢–û–ü-–°–õ–Ü–í: {len(filtered_tokens)} —Ç–æ–∫–µ–Ω—ñ–≤")
        print(f"   –ü—Ä–∏–∫–ª–∞–¥: {filtered_tokens[:10]}")

        # 3. –°—Ç–µ–º—ñ–Ω–≥
        from nltk.stem import PorterStemmer
        stemmer = PorterStemmer()
        stemmed_tokens = [stemmer.stem(word.lower()) for word in filtered_tokens]
        print(f"3. –°–¢–ï–ú–Ü–ù–ì: {len(stemmed_tokens)} —Ç–æ–∫–µ–Ω—ñ–≤")
        print(f"   –ü—Ä–∏–∫–ª–∞–¥: {stemmed_tokens[:10]}")

        # 4. –õ–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—è
        from nltk.stem import WordNetLemmatizer
        lemmatizer = WordNetLemmatizer()
        lemmatized_tokens = [lemmatizer.lemmatize(word.lower()) for word in filtered_tokens]
        print(f"4. –õ–ï–ú–ê–¢–ò–ó–ê–¶–Ü–Ø: {len(lemmatized_tokens)} —Ç–æ–∫–µ–Ω—ñ–≤")
        print(f"   –ü—Ä–∏–∫–ª–∞–¥: {lemmatized_tokens[:10]}")

        # 5. –ß–∞—Å—Ç–æ—Ç–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
        from nltk import FreqDist
        freq_dist = FreqDist(lemmatized_tokens)
        print(f"5. –¢–û–ü-10 –ù–ê–ô–ß–ê–°–¢–Ü–®–ò–• –°–õ–Ü–í:")
        for i, (word, freq) in enumerate(freq_dist.most_common(10), 1):
            print(f"   {i:2}. {word:15} - {freq:2} —Ä–∞–∑—ñ–≤")

        # 6. –°–ø—Ä–æ—â–µ–Ω–µ POS-—Ç–µ–≥—É–≤–∞–Ω–Ω—è (–±–µ–∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏—Ö —Ç–µ–≥—ñ–≤)
        print(f"6. –ü–†–û–°–¢–ï POS-–¢–ï–ì–£–í–ê–ù–ù–Ø:")
        pos_examples = {
            'intelligence': 'NOUN',
            'learning': 'NOUN',
            'machine': 'NOUN',
            'artificial': 'ADJ',
            'demonstrated': 'VERB',
            'advanced': 'ADJ'
        }
        for word, pos in list(pos_examples.items())[:6]:
            print(f"   {word:15} -> {pos}")

        return stemmed_tokens, lemmatized_tokens, freq_dist, filtered_tokens

    except Exception as e:
        print(f"–ü–û–ú–ò–õ–ö–ê: {e}")
        print("–ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –∑ –¥–æ—Å—Ç—É–ø–Ω–∏–º–∏ —Ñ—É–Ω–∫—Ü—ñ—è–º–∏...")
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø—É—Å—Ç—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        return [], [], FreqDist([]), []


def visualize_results(lemmatized_tokens, freq_dist):
    """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É"""

    print("\n" + "=" * 60)
    print("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í")
    print("=" * 60)

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # 1. –•–º–∞—Ä–∞ —Å–ª—ñ–≤
    if lemmatized_tokens:
        wordcloud = WordCloud(width=400, height=300, background_color='white',
                              max_words=50).generate(' '.join(lemmatized_tokens))
        axes[0, 0].imshow(wordcloud, interpolation='bilinear')
        axes[0, 0].set_title('–•–ú–ê–†–ê –°–õ–Ü–í', fontsize=14, fontweight='bold', pad=20)
        axes[0, 0].axis('off')
    else:
        axes[0, 0].text(0.5, 0.5, '–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó',
                        ha='center', va='center', fontsize=12)
        axes[0, 0].set_title('–•–ú–ê–†–ê –°–õ–Ü–í', fontsize=14, fontweight='bold')
        axes[0, 0].axis('off')

    # 2. –¢–æ–ø-15 —Å–ª—ñ–≤ –∑–∞ —á–∞—Å—Ç–æ—Ç–æ—é
    if freq_dist and len(freq_dist) > 0:
        top_words = dict(freq_dist.most_common(15))
        words = list(top_words.keys())
        frequencies = list(top_words.values())

        bars = axes[0, 1].barh(words, frequencies, color='skyblue')
        axes[0, 1].set_title('–¢–û–ü-15 –°–õ–Ü–í –ó–ê –ß–ê–°–¢–û–¢–û–Æ', fontsize=14, fontweight='bold', pad=20)
        axes[0, 1].set_xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤—Ö–æ–¥–∂–µ–Ω—å')

        # –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞ —Å—Ç–æ–≤–ø—Ü—ñ
        for bar, freq in zip(bars, frequencies):
            axes[0, 1].text(bar.get_width() + 0.1, bar.get_y() + bar.get_height() / 2,
                            str(freq), ha='left', va='center')
    else:
        axes[0, 1].text(0.5, 0.5, '–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó',
                        ha='center', va='center', fontsize=12)
        axes[0, 1].set_title('–¢–û–ü-15 –°–õ–Ü–í', fontsize=14, fontweight='bold')
        axes[0, 1].axis('off')

    # 3. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç—É
    axes[1, 0].axis('off')
    stats_text = """
–°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–ï–ö–°–¢–£:

 –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
‚Ä¢ –°–∏–º–≤–æ–ª–∏: {chars}
‚Ä¢ –°–ª–æ–≤–∞: {words}
‚Ä¢ –†–µ—á–µ–Ω–Ω—è: ~{sentences}

 –ü—ñ—Å–ª—è –æ–±—Ä–æ–±–∫–∏:
‚Ä¢ –¢–æ–∫–µ–Ω–∏: {tokens}
‚Ä¢ –£–Ω—ñ–∫–∞–ª—å–Ω—ñ —Å–ª–æ–≤–∞: {unique}
‚Ä¢ –ù–∞–π–¥–æ–≤—à–µ —Å–ª–æ–≤–æ: {longest}

 –ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –æ–±—Ä–æ–±–∫–∏:
‚Ä¢ –í–∏–¥–∞–ª–µ–Ω–æ —Å—Ç–æ–ø-—Å–ª—ñ–≤: {removed}
‚Ä¢ –ó–±–µ—Ä–µ–∂–µ–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∏—Ö: {kept}%
    """.format(
        chars=len(text),
        words=len(text.split()),
        sentences=text.count('.') + text.count('!') + text.count('?'),
        tokens=len(lemmatized_tokens) if lemmatized_tokens else 0,
        unique=len(set(lemmatized_tokens)) if lemmatized_tokens else 0,
        longest=max(lemmatized_tokens, key=len) if lemmatized_tokens else 'N/A',
        removed=len(text.split()) - len(lemmatized_tokens) if lemmatized_tokens else 0,
        kept=round(len(lemmatized_tokens) / len(text.split()) * 100, 1) if lemmatized_tokens else 0
    )

    axes[1, 0].text(0.05, 0.95, stats_text, fontsize=11, verticalalignment='top',
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
    axes[1, 0].set_title('–°–¢–ê–¢–ò–°–¢–ò–ß–ù–ò–ô –ê–ù–ê–õ–Ü–ó', fontsize=14, fontweight='bold')

    # 4. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç–æ–¥—ñ–≤ –æ–±—Ä–æ–±–∫–∏
    if lemmatized_tokens:
        methods = ['–û—Ä–∏–≥—ñ–Ω–∞–ª', '–¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è', '–§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è', '–õ–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—è']
        counts = [
            len(text.split()),
            len(lemmatized_tokens) * 2,  # –ü—Ä–∏–±–ª–∏–∑–Ω–æ
            len(lemmatized_tokens) + 10,  # –ü—Ä–∏–±–ª–∏–∑–Ω–æ
            len(lemmatized_tokens)
        ]

        bars = axes[1, 1].bar(methods, counts, color=['lightblue', 'lightgreen', 'orange', 'lightcoral'])
        axes[1, 1].set_title('–ï–¢–ê–ü–ò –û–ë–†–û–ë–ö–ò –¢–ï–ö–°–¢–£', fontsize=14, fontweight='bold', pad=20)
        axes[1, 1].set_ylabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤')

        # –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞ —Å—Ç–æ–≤–ø—Ü—ñ
        for bar, count in zip(bars, counts):
            axes[1, 1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,
                            str(count), ha='center', va='bottom')
    else:
        axes[1, 1].text(0.5, 0.5, '–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è',
                        ha='center', va='center', fontsize=12)
        axes[1, 1].set_title('–ï–¢–ê–ü–ò –û–ë–†–û–ë–ö–ò', fontsize=14, fontweight='bold')
        axes[1, 1].axis('off')

    plt.tight_layout()
    plt.show()


def comparative_analysis(stemmed_tokens, lemmatized_tokens, filtered_tokens):
    """–ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –º–µ—Ç–æ–¥—ñ–≤ –æ–±—Ä–æ–±–∫–∏"""

    print("\n" + "=" * 60)
    print("–ü–û–†–Ü–í–ù–Ø–õ–¨–ù–ò–ô –ê–ù–ê–õ–Ü–ó –ú–ï–¢–û–î–Ü–í")
    print("=" * 60)

    if not filtered_tokens:
        print("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è")
        return

    # –°—Ç–≤–æ—Ä—é—î–º–æ DataFrame –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    comparison_data = []
    for i in range(min(8, len(filtered_tokens))):
        original = filtered_tokens[i]
        stemmed = stemmed_tokens[i] if i < len(stemmed_tokens) else "N/A"
        lemmatized = lemmatized_tokens[i] if i < len(lemmatized_tokens) else "N/A"

        comparison_data.append({
            '–û—Ä–∏–≥—ñ–Ω–∞–ª': original,
            '–°—Ç–µ–º—ñ–Ω–≥': stemmed,
            '–õ–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—è': lemmatized
        })

    # –í–∏–≤–æ–¥–∏–º–æ —Ç–∞–±–ª–∏—Ü—é –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    df = pd.DataFrame(comparison_data)
    print("–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç–æ–¥—ñ–≤ –æ–±—Ä–æ–±–∫–∏ —Å–ª—ñ–≤:")
    print(df.to_string(index=False))

    # –ê–Ω–∞–ª—ñ–∑ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
    print(f"\n–ê–ù–ê–õ–Ü–ó –ï–§–ï–ö–¢–ò–í–ù–û–°–¢–Ü:")
    print(f"‚Ä¢ –°—Ç–µ–º—ñ–Ω–≥ –∑–º–µ–Ω—à—É—î —Å–ª–æ–≤–∞ –¥–æ –∫–æ—Ä–µ–Ω—è: 'intelligence' ‚Üí 'intellig'")
    print(f"‚Ä¢ –õ–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—è –∑–±–µ—Ä—ñ–≥–∞—î —Å–ª–æ–≤–Ω–∏–∫–æ–≤—É —Ñ–æ—Ä–º—É: 'machines' ‚Üí 'machine'")
    print(f"‚Ä¢ –û–±–∏–¥–≤–∞ –º–µ—Ç–æ–¥–∏ –ø–æ–∫—Ä–∞—â—É—é—Ç—å —è–∫—ñ—Å—Ç—å –∞–Ω–∞–ª—ñ–∑—É —Ç–µ–∫—Å—Ç—É")


# –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è
def main():
    print("üöÄ –ó–ê–ü–£–°–ö –õ–ê–ë–û–†–ê–¢–û–†–ù–û–á –†–û–ë–û–¢–ò ‚Ññ10 - TEXT MINING")
    print("=" * 70)

    # –í–∏–∫–æ–Ω—É—î–º–æ –æ–±—Ä–æ–±–∫—É —Ç–µ–∫—Å—Ç—É
    stemmed, lemmatized, freq_dist, filtered = preprocess_text_nltk(text)

    # –í—ñ–∑—É–∞–ª—ñ–∑—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    visualize_results(lemmatized, freq_dist)

    # –ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –º–µ—Ç–æ–¥–∏
    comparative_analysis(stemmed, lemmatized, filtered)

    # –í–∏—Å–Ω–æ–≤–∫–∏
    print("\n" + "=" * 70)
    print("–í–ò–°–ù–û–í–ö–ò –¢–ê –†–ï–ó–£–õ–¨–¢–ê–¢–ò")
    print("=" * 70)
    print("‚úÖ 1. –¢–µ–∫—Å—Ç —É—Å–ø—ñ—à–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é NLTK")
    print("‚úÖ 2. –í–∏–∫–æ–Ω–∞–Ω–æ –≤—Å—ñ –µ—Ç–∞–ø–∏ Text Mining:")
    print("   - –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è (—Ä–æ–∑–±–∏—Ç—Ç—è –Ω–∞ —Å–ª–æ–≤–∞)")
    print("   - –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–æ–ø-—Å–ª—ñ–≤")
    print("   - –°—Ç–µ–º—ñ–Ω–≥ (–∑–≤–µ–¥–µ–Ω–Ω—è –¥–æ –∫–æ—Ä–µ–Ω—è)")
    print("   - –õ–µ–º–∞—Ç–∏–∑–∞—Ü—ñ—è (—Å–ª–æ–≤–Ω–∏–∫–æ–≤–∞ —Ñ–æ—Ä–º–∞)")
    print("‚úÖ 3. –°—Ç–≤–æ—Ä–µ–Ω–æ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É:")
    print("   - –•–º–∞—Ä–∞ —Å–ª—ñ–≤")
    print("   - –ß–∞—Å—Ç–æ—Ç–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª")
    print("   - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ–∫—Å—Ç—É")
    print("   - –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç–æ–¥—ñ–≤")
    print("‚úÖ 4. –í–∏–∑–Ω–∞—á–µ–Ω–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ —Ç–µ–∫—Å—Ç—É:")

    if freq_dist and len(freq_dist) > 0:
        top_words = [word for word, freq in freq_dist.most_common(5)]
        print(f"   {', '.join(top_words)}")

    print("\nüìä Text Mining –¥–æ–∑–≤–æ–ª—è—î –∞–≤—Ç–æ–º–∞—Ç–∏–∑—É–≤–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑ —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö")
    print("—Ç–∞ –≤–∏—è–≤–ª—è—Ç–∏ –∫–ª—é—á–æ–≤—ñ —Ç–µ–Ω–¥–µ–Ω—Ü—ñ—ó —Ç–∞ –∑–∞–∫–æ–Ω–æ–º—ñ—Ä–Ω–æ—Å—Ç—ñ!")


# –ó–∞–ø—É—Å–∫–∞—î–º–æ –ø—Ä–æ–≥—Ä–∞–º—É
if __name__ == "__main__":
    main()